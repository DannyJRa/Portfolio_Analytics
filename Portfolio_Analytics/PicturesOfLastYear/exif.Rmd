---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Source: https://www.r-bloggers.com/extracting-exif-data-from-photos-using-r/
```{r}
library(exifr)
library(dplyr)
library(leaflet)
```


Now set your working director the a folder that holds the photos in questions. We can then get the names of all the photos straight into R's memory like this:

```{r}

#files <- list.files(path="Pictures",pattern = "*.JPG")
#dat <- exifr(files)

dat <- read_exif(path="Pictures",recursive=T)
dat$Rating
```
You can get all the files and then loop using lapply and apply whatever function you want to apply as follows:
```{r}
files <- list.files(path="path/to/dir", pattern="*.txt", full.names=T, recursive=FALSE)
lapply(files, function(x) {
    t <- read.table(x, header=T) # load file
    # apply function
    out <- function(t)
    # write to file
    write.table(out, "path/to/output", sep="\t", quote=F, row.names=F, col.names=T)
})
```



The pattern argument ensures we just grab the jpegs from the folder and nothing else.
Neat, we have our exif info as a dataframe. Now let's select just the useful columns:


```{r}
dat2 <- select(dat,
	SourceFile, DateTimeOriginal,
	GPSLongitude, GPSLatitude,
	GPSTimeStamp)

	write.csv(dat2, 'Exifdata.csv',
	row.names = F)
```


NB the select function comes from the dplyr package. You can do this with base R too, but I prefer dplyr. (You can get my dataframe here)

You can make a quick map of locations like this:

```{r}
plot(dat$GPSLongitude, dat$GPSLatitude)
```
Make an interactive map
Interactive web maps are easy with the leaflet package. We can plot the same points over and ESRI provided satellite image like this:
```{r}


leaflet(dat2) %>%
addProviderTiles("Esri.WorldImagery") %>%
addMarkers(~ GPSLongitude, ~ GPSLatitude)  
```



# Part 2
SOurce: http://www.seascapemodels.org/rstats/2016/11/23/mapping-abundance-photos.html

```{r}

```
Getting started

First up here are the packages we need:

```{r}
library(leaflet)
library(exifr)
```


The next list of packages is optional, but I will use here to make data processing more convenient:
```{r}

library(readr) #for the updated version of read.csv
library(dplyr) #for data wrangling
library(stringr) #for wrangling strings
```

Finally to get some nice colours:

```{r}
library(RColorBrewer)
```


Now load in the data. I am using read_csv which comes from the readr package. read_csv is a modernised version of the base R function read.csv. Here are the exif data and associated oyster counts if you are repeating this example.
```{r}
edat <- read_csv('Exifdata.csv')
odat <- read_csv('oysters_data.csv')
n <- nrow(odat) #number of sample sites
```


Joining the locations to the counts

Now we need to join our counts of oysters to the locations. We can match by the picture numbers, which I recorded when I counted the oysters. First up we need to clean the clean the picture numbers in the exif data to remove the leading 'DSC_'. We do this using mutate from dplyr and str_extract from stringr to extract the first four digits in each picture's filename.

```{r}
edat<- edat %>%
mutate(pic_num =
	as.numeric(
		str_extract(SourceFile, '\\d\\d\\d\\d')
		))
```


If you haven't seen %>% before it is a 'pipe' that puts the dataframe edat into the first argument of our mutate function. The pipe isn't essential, but I think it improves readibility. See Wickham's book for more guidance on pipes.

Now we can join the oysters and pictures. left_join will automatically identify pic_num to join on, because it is the common column across both dataframes:

```{r}
dat <- odat %>%
  left_join(edat)


```



Create the content for the pop-up image

To create the pop-up you need to know a little bit of html. There is plenty of helpful guides on the web and html is pretty simple to learn. The pop-up will show an image and some text. Here we define the content:

```{r}
content <- paste(sep = "<br/>",
  	"<img src='http://www.seascapemodels.org/images/intertidal_scene.JPG'
   style='width:230px;height:300px;'>",
  	"The intertidal zone",
  	"at Hornby Island"
)
content

```


In short we have specified a string with some html code. <br/> html tag creates a new line. The <img> tag specifies the insertion of an image (the link gives the image's location, which is on my webpage. We have also set the width and height of the image. Then we follow with a bit of text explaining the image.

We will use content in our leaflet map below.

We also need the location to place our pop-up. Because the image is gps tagged, we can extract the gps locations from the exif data. Download the image from the link given above in content then you can get the exif data associated with it like this:
```{r}
scene <- read_exif('intertidal_scene.JPG')
x1 <- scene $GPSLongitude
y1 <- scene $GPSLatitude
```
# Color

```{r}
brks <- c(0,1, 5, 10, 20, 70)
ncol <- length(brks)-1
oystercols <- c('grey20',brewer.pal(ncol-1, 'Purples'))
pal <- colorBin(oystercols, dat$oysters_live, bins = brks)
```


We now have coordinates for the pop-up in our leaflet map.

#Mapoutput


```{r}
mapout <- leaflet(dat) %>%
#Use satellite image as base
addProviderTiles("Esri.WorldImagery") %>%
setView(lng = x1, lat = y1, zoom = 16) %>%
#Add markers for oyster quadrats
addCircleMarkers(~ GPSLongitude, ~ GPSLatitude,
	color = 'white',opacity =1, weight = 1,
	fillColor = ~pal(oysters_live),
	popup = as.character(dat$oysters_live),
	fillOpacity = 0.8,
	radius = 6) %>% # add a popup for number of oysters
	#Add marker showing a picture of the survey site
	addMarkers(x1, y1, popup = content,
		options = markerOptions(opacity = 0.9, draggable=T)) %>%
		#Add a legend
		addLegend("topright", pal = pal,
		values = brks,
		title = "Number of live oysters
		<a href = 'https://en.wikipedia.org/wiki/Pacific_oyster' target = '_blank'> (Crassostrea gigas)</a>",
		opacity = 1)
mapout
```



# Part 3
http://www.seascapemodels.org/rstats/2017/02/22/spatial-statistics-photos.html



First up we need to load some packages:

```{r}
#Source: http://www.r-inla.org/
#install.packages("INLA", repos="https://inla.r-inla-download.org/R/testing", dep=TRUE)
    library(readr)
    library(INLA) 

    library(sp)
    library(raster)
    library(rgeos)

library(rgdal)
```


The INLA package is crucial here. We will be using INLA to perform Bayesian Kriging. In non-technical terms, Bayesian kriging lets us fill in the gaps between observations of oyster counts by interpolating from nearby points.

Getting the data in the right form

Before we begin, we need to load in the data, and transform it into a spatial data frame. In particular, it is important to transform the lon-lats into UTM coordinates. UTM coordinates allow us to measure distances between sample sites in metres:
```{r}
    dat <- read_csv('Oysters_merged.csv')
    n <- nrow(dat)
    utmproj <- "+proj=utm +zone=10 +north +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
    spdat <- dat
    coordinates(spdat) <- ~ GPSLongitude + GPSLatitude
    proj4string(spdat) <- "+init=epsg:4326"
    spdf <- spTransform(spdat, CRS(utmproj))
    plot(spdf)
```




You should get a plot like that pictured just of the sample sites.

Creating an INLA "Mesh"

Now we can start building the specialised data-structures we need for the INLA package. First up, we have to build what's called a mesh. This is a Delaunay triangulation built around our data points. It accounts for the non-regular spatial structuring of the sampling (I wandered around on the rocky shore throwing the quadrat at intervals).

Below we provide two parameters to max.edge, this will enable us to buffer the edges to obtain a slightly larger spatial domain. Buffering is important because the INLA approximation will be inaccurate at the edges of the spatial domain.

```{r}
max.edge.length <- c(25, 40)
loc1 <- as.matrix(coordinates(spdf))
mesh <- inla.mesh.2d(loc=loc1, max.edge = max.edge.length, offset=1, cutoff = 5)

plot(mesh)
plot(spdf, add = T, col = 'red')

```



You can play with the edge length, offset and cutoff parameters to vary how the triangulations turns out.

You can get more guidance at the INLA page and in this tutorial. This tutorial also has some very excellent advice about how to choose boundaries and triangle sizes.

An important step is to check that our maximum edge lengths are less than the estimated range (otherwise it is pointless including a spatial effect!). We will check the range size below, once we have fitted the model.

You might like to think of a-prior reasons for modelling a certain range, for instance, what is a reasonable spatial scale for 'clumping' of oyster patches?

Build INLA stack

Now we need to implement a few more steps to build up an appropriate 'stack' of data for INLA. Note that I have transformed oyster count to presence - absence. We will build a binomial model and just predict whether any given quadrat had oysters or not.

```{r}
A.data <- inla.spde.make.A(mesh, loc1)
spde <- inla.spde2.matern(mesh, alpha = 2)
spdf$presabs <- ifelse(spdf$oysters_live>0, 1, 0)

stk <- inla.stack(data=list(y=spdf$presabs), A=list(A.data, 1),
                  effects=list(s=1:mesh$n, intercept=rep(1, n)),
                  remove.unused = FALSE, tag = "est")
```


Note that here we choose alpha = 2. alpha is a smoothness parameter that must be 0 <= alpha <= 2, higher values will give smoother spatial interpolation. Also note that we have to specify 'data' for the intercept (just a vector of 1).

The tag = "est" argument just gives a name tag to the fitted model. In other applications you might want to join the estimation stack with a prediction stack (just build two stacks, one where the response is NA and then join them by applying inla.stack to both).

Model

Now, after that considerable amount of prepartion, we get to fitting the model. We use standard R syntax for the formula, with the addition of the f() term to specify a function.
```{r}

    formula <- y ~ 0 + intercept + f(s, model=spde)

    mod <- inla(formula, data=inla.stack.data(stk),
            control.predictor=list(A = inla.stack.A(stk)),
            family = 'binomial')
```

We can extract summary parameters using summary. Here we are interested in the random effects, it is good practice to call inla.hyperpar before we inspect random effects:

```{r}
hyper <- inla.hyperpar(mod)
summary(hyper)
```


Checking the range parameter

We should also check that our triangle edge length is less than the estimated spatial range. We can do that by making a call to the model object, asking for the estimated parameters for the random field

```{r}
rf <- inla.spde.result(inla = mod, name = "s",
spde = spde, do.transf = TRUE)

plot(rf$marginals.range[[1]], type = "l",
 xlab = "Range (m)", ylab = "Density",
 xlim = c(0, 1000))
abline(v = max.edge.length[2], col = 'red')

```



The red line shows our edge length parameter, which is well less than the model estimate for the spatial range.

Model predictions

Now we have a fitted model, we can extracted predicted probabilities of oyster presence and map them. First transform the predictions from the logit scale back to probabilities:

```{r}
  ypred <- exp(mod$summary.random$s$mean)/(1 + exp(mod$summary.random$s$mean))
```


Then we can turn our predictions into a spatial points, that can be be mapped onto a raster for plotting:

```{r}
extent <- extent(spdf)
xdims <- 100; ydims <- 200
xlim <- c(extent[1], extent[2]); ylim = c(extent[3], extent[4]);
proj <- inla.mesh.projector(mesh, xlim = xlim, ylim = ylim, dims=c(xdims, ydims))
field.proj <- inla.mesh.project(proj, ypred)

datpred <- data.frame(x = rep(proj$x, ydims), y = rep(proj$y, each = xdims), pred = as.numeric(field.proj))
coordinates(datpred) <- ~x + y

proj4string(datpred) <- utmproj
dat3 <- spTransform(datpred, crs("+init=epsg:4326"))

r <- raster(extent(dat3), ncols = xdims, nrows= ydims, crs = crs(spdat))
icell <- cellFromXY(r, dat3)

r[icell] <- as.numeric(dat3$pred)
#And finally the plot:

cols <- RColorBrewer::brewer.pal(9, "Purples")
plot(r, col = cols)
points(spdf)
plot(spdat, add = TRUE)
```




So it looks like oysters are more common in the southern portion of the survey area.

But note that our predictions extent a fair way from the points. We might want to redo these with a more constrained spatial region. In general we should't trust predictions that are more than ~ 1 spatial range from the edge of our sampled region (in this case the posterior estimate for range was 181m).

Finally, we could map this onto a satellite image to get a better view of what is going on. I am just using the same leaflet code as before:

```{r}
library(leaflet)
brks <- seq(0,1, by = 0.1)
ncol <- length(brks)
oystercols <- RColorBrewer::brewer.pal(min(ncol, 9), 'Reds')
pal <- colorBin(oystercols, dat$oysters_live, bins = brks)

x1 <- -124.5997
y1 <- 49.52848

leaflet() %>%
addProviderTiles("Esri.WorldImagery") %>%

setView(lng = x1, lat = y1, zoom = 16) %>%
addRasterImage(r, opacity = 0.8, color = pal) %>%
addCircleMarkers(lng = dat$GPSLongitude, lat = spdat$GPSLatitude,
radius = 4) %>%
addLegend("topright", pal = pal,
values = brks,
title = "Chance of live oysters <a href = 'https://en.wikipedia.org/wiki/Pacific_oyster' target = '_blank'> (Crassostrea gigas)</a>",
opacity = 1)
```

