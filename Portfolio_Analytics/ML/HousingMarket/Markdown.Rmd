---
title: "Untitled"
output: html_document
---

SOURCE: https://www.kaggle.com/camnugent/introduction-to-machine-learning-in-r-tutorial/notebook


```{r}
library(tidyverse)
library(reshape2)
```

# Step 1. Load in the data.

```{r, echo=FALSE}
housing = read.csv('1_Data/housing.csv')
```

First thing I always do is use the head command to make sure the data isn't weird and looks how I expected.

```{r, echo=FALSE}
head(housing)
summary(housing)
```


```{r, echo=FALSE}
head(housing)
```

```{r, echo=FALSE}
par(mfrow = c(2, 5))

colnames(housing)

ggplot(data = melt(housing), mapping = aes(x = value)) +
    geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
```

Things I see from this:

There are some housing blocks with old age homes in them.
The median house value has some weird cap applied to it causing there to be a blip at the rightmost point on the hist. There are most definitely houses in the bay area worth more than 500,000... even in the 90s when this data was collected!
We should standardize the scale of the data for any non-tree based methods. As some of the variables range from 0-10, while others go up to 500,000
We need to think about how the cap on housing prices can affect our prediction... may be worth removing the capped values and only working with the data we are confident in.


# Step 2. Clean the data


Impute missing values

```{r}
housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms, na.rm = TRUE)
```

Fix the total columns - make them means

```{r}
housing$mean_bedrooms = housing$total_bedrooms / housing$households
housing$mean_rooms = housing$total_rooms / housing$households

drops = c('total_bedrooms', 'total_rooms')

housing = housing[, !(names(housing) %in% drops)]
```

Turn categoricals into booleans

Below I do the following:

Get a list of all the categories in the 'ocean_proximity' column
Make a new empty dataframe of all 0s, where each category is its own colum
Use a for loop to populate the appropriate columns of the dataframe
Drop the original column from the dataframe.
This is an example of me coding R with a python accent... I would love comments about how to do this more cleanly in R!

```{r}
categories = unique(housing$ocean_proximity)
#split the categories off
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)
for (cat in categories)
{
    cat_housing[, cat] = rep(0, times = nrow(cat_housing))
}
head(cat_housing) #see the new columns on the right
```

```{r}
for (i in 1:length(cat_housing$ocean_proximity))
{
    cat = as.character(cat_housing$ocean_proximity[i])
    cat_housing[, cat][i] = 1
}

head(cat_housing)


```

```{r}
cat_columns = names(cat_housing)
keep_columns = cat_columns[cat_columns != 'ocean_proximity']
cat_housing = select(cat_housing, one_of(keep_columns))

tail(cat_housing)


```


## Scale the numerical variables

ote here I scale every one of the numericals except for 'median_house_value' as this is what we will be working to predict. The x values are scaled so that coefficients in things like support vector machines are given equal weight, but the y value scale doen't affect the learning algorithms in the same way (and we would just need to re-scale the predictions at the end which is another hassle).

```{r}
colnames(housing)
drops = c('ocean_proximity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)

scaled_housing_num = scale(housing_num)
head(scaled_housing_num)
```


## Merge the altered numerical and categorical dataframes

```{r}
cleaned_housing = cbind(cat_housing, scaled_housing_num, median_house_value = housing$median_house_value)
head(cleaned_housing)


```



# Step 3. Create a test set of data

```{r}
set.seed(1738) # Set a random seed so that same sample can be reproduced in future runs

sample = sample.int(n = nrow(cleaned_housing), size = floor(.8 * nrow(cleaned_housing)), replace = F)
train = cleaned_housing[sample,] #just the samples
test = cleaned_housing[-sample,] #everything but the samples
```

I like to use little sanity checks like the ones below to make sure the manipulations have done what I want. With big dataframes you need find ways to be sure that don't involve looking at the whole thing every step!

Note that the train data below has all the columns we want, and also that the index is jumbled (so we did take a random sample). The second check makes sure that the length of the train and test dataframes equals the length of the dataframe they were split from, which shows we didn't lose data or make any up by accident!

```{r}
head(train)
nrow(train) + nrow(test) == nrow(cleaned_housing)
```


# Step 4. Test some predictive models.

We start here with just a simple linear model using 3 of the avaliable predictors. Median income, total rooms and population. This serves as an entry point to introduce the topic of cross validation and a basic model. We want a model that makes good predictions on data that it has not seen before. A model that explains the variation in the data it was trained on well, but does not generalize to external data is referred to as being overfit. You may thin "that's why we split off some test data!" but we don't want to repeatedly assess against our test set, as then the model can just become overfit to that set of data thus moving and not solving the problem.

So here we do cross validation to test the model using the training data itself. Our K is 5, what this means is that the training data is split into 5 equal portions. One of the 5 folds is put to the side (as a mini test data set) and then the model is trained using the other 4 portions. After that the predictions are made on the folds that was withheld, and the process is repeated for each of the 5 folds and the average predictions produced from the iterations of the model is taken. This gives us a rough understanding of how well the model predicts on external data!

```{r}
library('boot')
? cv.glm # note the K option for K fold cross validation
glm_house = glm(median_house_value ~ median_income + mean_rooms + population, data = cleaned_housing)
k_fold_cv_error = cv.glm(cleaned_housing, glm_house, K = 5)
k_fold_cv_error$delta
```

The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate.

```{r}
glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]
glm_cv_rmse #off by about $83,000... it is a start
```


```{r}
names(glm_house) #what parts of the model are callable?

glm_house$coefficients
```

Since we scaled the imputs we can say that of the three we looked at, median income had the biggest effect on housing price... but I'm always very careful and google lots before intrepreting coefficients!

## Random forest

```{r}
library('randomForest')
```

```{ r }
? randomForest
names(train)

set.seed(1738)

train_y = train[, 'median_house_value']
train_x = train[, names(train) != 'median_house_value']

head(train_y)
head(train_x)



#some people like weird r format like this... I find it causes headaches
#rf_model = randomForest(median_house_value~. , data = train, ntree =500, importance = TRUE)
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)
names(rf_model) #these are all the different things you can call from the model.

rf_model$importance
```

Percentage included mean squared error is a measure of feature importance. It is defined as the measure of the increase in mean squared error of predictions when the given variable is shuffled, thereby acting as a metric of that given variable’s importance in the performance of the model. So higher number == more important predictor.

The out-of-bag (oob) error estimate
In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:

Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.

```{r}
oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions
#you may have noticed that this is avaliable using the $mse in the model options.
#but this way we learn stuff!
train_mse = mean(as.numeric((oob_prediction - train_y) ^ 2))
oob_rmse = sqrt(train_mse)
oob_rmse
```

So even using a random forest of only 1000 decision trees we are able to predict the median price of a house in a given district to within $49,000 of the actual median house price. This can serve as our bechmark moving forward and trying other models.

How well does the model predict on the test data?

```{r}
test_y = test[, 'median_house_value']
test_x = test[, names(test) != 'median_house_value']


y_pred = predict(rf_model, test_x)
test_mse = mean(((y_pred - test_y) ^ 2))
test_rmse = sqrt(test_mse)
test_rmse
```

Well that looks great! Our model scored roughly the same on the training and testing data, suggesting that it is not overfit and that it makes good predictions.

# Step 5. Next Steps

So above we have covered the basics of cleaning data and getting a machine learning algorithm up and running in R. But I've on purpose left some room for improvement.

The obvious way to improve the model is to provide it with better data. Recall our columns:

longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity

Suggestions on ways to improve the results
Why not use your R skills to build new data! One suggestion would be to take the longitude and latitude and work with these data. You could try to find things like 'distance to closest city with 1 million people' or other location based stats. This is called feature engineering and data scientists get paid big bucks to do it effectively!

You may also wish to branch out and try some other models to see if they improve over the random forest benchmark we have set. Note this is not an exhaustive list but a starting point

Tree based methods:

gradient boosting - library(gbm) extreme gradient boosting - library(xgb)

Other fun methods: support vevtor machines - library(e1071) neural networks - library(neuralnet)

Hyperparameters and Grid search
When tuning models the next thing to worry about is the hyperparameters. All this means is the different options you pass into a model when you initialze it. i.e. the hyperparameter in out random forest model was n_tree = x, we chose x = 500, but we could have tried x = 2500, x = 1500, x = 100000 etc.

Grid search is a common method to find the best combination of hyperparameters (as there are often more than the 1 we see in the random forest example!). Essentially this is where you make every combination of a set of paramaters and run a cross validation on each set, seeing which set gives the best predictions. An alternative is random search. When the number of hyperparameters is high then the computational load of a full grid search may be too much, so a random search takes a subset of the combinations and finds the best one in the random sample (sounds like a crapshoot but it actually works well!). These methods can be implemented easily using a for loop or two... there are also packages avaliable to help with these tasks.

Here we exit the scope of what I can cover in a short tutorial, look at the r package 'caret' it has great functions for streamling things like grid searches for the best parameters. http://caret.r-forge.r-project.org/




# Machine Learning in R: part 2


## 2a. Last week's random forest model
```{r}
########
# Random Forest Model
########
library(randomForest)
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)

names(rf_model) #these are all the different things you can call from the model.

importance_dat = rf_model$importance
importance_dat

sorted_predictors = sort(importance_dat[, 1], decreasing = TRUE)
sorted_predictors

oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions

#you may have noticed that this is avaliable using the $mse in the model options.
#but this way we learn stuff!
train_mse = mean(as.numeric((oob_prediction - train_y) ^ 2))
oob_rmse = sqrt(train_mse)
oob_rmse


y_pred_rf = predict(rf_model, test_x)
test_mse = mean(((y_pred_rf - test_y) ^ 2))
test_rmse = sqrt(test_mse)
test_rmse # ~48620


```

## 2b. Gradient Boosting

Gradient boosting is an ensemble supervised machine learning model that builds up the concept of the random forest algorithm we explored last week. Recall that for a random forest we spawned 500 decision trees and took the mean of their predictions to get a 'wisdom of the crowd' effect and arrive at a more accurate prediction than any one tree would provide.

Here we use the Extreme Gradient Boosting library to implement this in R. Note the 'Extreme' in extreme gradient boosting refers to the computational efficiency. The algorithm could more accurately could be described as 'regularized gradient boosting'.

### Gradient Boosting - Algorithm details

Extreme gradient boosting also builds a forest of trees, but does so in an additive manner. The algorithm iteratively builds trees that minimize the error, and thereby descends towards an optimal set of predictive trees. Already learned trees are kept, and new trees are added one after another to minimize the objective function (error in predictions). The trees are grown sequentially: each tree is grown using information from previously grown trees. Each tree is fit on a modified version of the original data set based on the previous trees built.

The trees are accompanied by a regularization paramater to avoid overfit.

One difference between boosting and random forests: in boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient (less splits and depth).

```{r}
######
# XG Boost
######
# see the docs: http://cran.fhcrc.org/web/packages/xgboost/vignettes/xgboost.pdf
library(xgboost)

#put into the xgb matrix format
dtrain = xgb.DMatrix(data = as.matrix(train_x), label = train_y)
dtest = xgb.DMatrix(data = as.matrix(test_x), label = test_y)

# these are the datasets the rmse is evaluated for at each iteration
watchlist = list(train = dtrain, test = dtest)

# try 1 - off a set of paramaters I know work pretty well for most stuff

bst = xgb.train(data = dtrain,
                max.depth = 8,
                eta = 0.3,
                nthread = 2,
                nround = 1000,
                watchlist = watchlist,
                objective = "reg:linear",
                early_stopping_rounds = 50,
                print_every_n = 500)


```

So our first run there gets a rmse of $47723, an improvement over our benchmark model. That isn't the end of the story though, we can try to squeak out further improvements through 'hyperparameter tuning'. A hyperparameter is one of the mutable options that we pass to the algorithm along with our data.

## 2c. Tuning the algorithm - hyperparameters for xgboost

Boosting has 3 tuning paramaters that we can focus on

The number of trees. Here we use a good trick, instead of specifying an exact number, we give the algorithm a big number (nround = 10000) and the param (early_stopping_rounds = 50). This effectively means: 'keep iteratively growing trees until you have 10,000 of them, or stop early if the scores haven't improved for the last 50 rounds'.
The shrinkage parameter λ (eta in the params), a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small λ can require using a very large value of B in order to achieve good performance.
The number of splits in each tree, which controls the complexity of the boosted ensemble (controlled with max.depth).
Here we try a 'slower learning' model. The up and down weights for each iteration are smaller we also use more iterations to account for the fact that the model will take longer to learn.




```{r}
bst_slow = xgb.train(data = dtrain,
                        max.depth = 5,
                        eta = 0.01,
                        nthread = 2,
                        nround = 10000,
                        watchlist = watchlist,
                        objective = "reg:linear",
                        early_stopping_rounds = 50,
                        print_every_n = 500)


```

```{r}
rf_benchmark = 48392

bst_slow$best_score / rf_benchmark


```

Things to note:

6009 iterations were run, and it backtracked to 5959 to get the best one thanks to our early stopping rounds parameter.
So that is an improvement of ~6.5% in rmse error over last week. Yay lets go home. Wait! What we have done here is fit to the training set and the training set at the same time (leading to model overfit).


### Problems to address
1. We need to work with a validation set and only at the end evaluate the model performance against the test set.
Remember our test set should be withheld to evaluate the model on data it hasn't seen. By iteratively checking the train and test rmse above, we have violated that rule and so we cannot say how accurate our model is on data it hasn't seen. To do this we need a validation set (essentially a second test set) that the model can peek at after each iteration to see how well it is performing or external data, then with the final version we can make the assessment vs. the test set.

2 .If we kept tweaking one hyperparameter, waiting to see the result and then repeating the process we will be here forever. We need to speed this up in a systematic fashion


## 2d. A validation set

validation set - Another subset of our data that is witheld from the training algorithm, but compared against at each iteration to see how the model is performing.

Here we make this through the same method as the test set, by sampling 20% of the remaining training set and passing this into the xgboost watchlist. The algorithm will watch the rmse of the training (which it can learn from) and the validation (which it can't learn parameters from, only see the rmse outcome) and continue until one is no longer improving
```{r}
####
# Proper use - validation set
####


sample = sample.int(n = nrow(train), size = floor(.8 * nrow(train)), replace = F)

train_t = train[sample,] #just the samples
valid = train[-sample,] #everything but the samples

train_y = train_t[, 'median_house_value']

#if tidyverse was used, dplyr pull function solves the problem:
#train_y = pull(train_t, median_house_value)
train_x = train_t[, names(train_t) != 'median_house_value']

valid_y = valid[, 'median_house_value']
valid_x = valid[, names(train_t) != 'median_house_value']

train_y[1:10]



```


```{r}
gb_train = xgb.DMatrix(data = as.matrix(train_x), label = train_y)
gb_valid = xgb.DMatrix(data = as.matrix(valid_x), label = valid_y)
#in jupyter the label needs to be in an as.matrix() or I get an error? subtle and annoying differences

# train xgb, evaluating against the validation
watchlist = list(train = gb_train, valid = gb_valid)


```

We then run the xgboost algorithm again and after training we evaluate on the test data.
```{r}
bst_slow = xgb.train(data = gb_train,
                        max.depth = 10,
                        eta = 0.01,
                        nthread = 2,
                        nround = 10000,
                        watchlist = watchlist,
                        objective = "reg:linear",
                        early_stopping_rounds = 50,
                        print_every_n = 500)


```


```{r}
# recall we ran the following to get the test data in the right format:
# dtest = xgb.DMatrix(data =  as.matrix(test_x), label = test_y)
# here I have it with the label taken off, just to remind us its external data xgb would ignore the label though during predictions
dtest = xgb.DMatrix(data = as.matrix(test_x))

#test the model on truly external data

y_hat_valid = predict(bst_slow, dtest)

test_mse = mean(((y_hat_valid - test_y) ^ 2))
test_rmse = sqrt(test_mse)
test_rmse


```


```{r}
test_rmse / rf_benchmark


```

This is higher then on the first run, but we can be confident that the improved score is not due to overfit thanks to our use of a validation set! A lower rmse isn't necessarily better if it comes at the cose of overfit, we now have more confidence in external predictions.


## 3a. Grid Search to find the best hyperparameter combinations
```{r}
###
# Grid search first principles 
###

max.depths = c(7, 9)
etas = c(0.01, 0.001)

best_params = 0
best_score = 0

count = 1
for (depth in max.depths)
{
    for (num in etas)
    {

        bst_grid = xgb.train(data = gb_train,
                                max.depth = depth,
                                eta = num,
                                nthread = 2,
                                nround = 10000,
                                watchlist = watchlist,
                                objective = "reg:linear",
                                early_stopping_rounds = 50,
                                verbose = 0)

        if (count == 1)
        {
            best_params = bst_grid$params
            best_score = bst_grid$best_score
            count = count + 1
        }
        else if (bst_grid$best_score < best_score)
        {
            best_params = bst_grid$params
            best_score = bst_grid$best_score
        }
    }
}

best_params
best_score
```


```{r}
# max_depth of 9, eta of 0.01
bst_tuned = xgb.train(data = gb_train,
                        max.depth = 7,
                        eta = 0.01,
                        nthread = 2,
                        nround = 10000,
                        watchlist = watchlist,
                        objective = "reg:linear",
                        early_stopping_rounds = 50,
                        print_every_n = 500)

y_hat_xgb_grid = predict(bst_tuned, dtest)

test_mse = mean(((y_hat_xgb_grid - test_y) ^ 2))
test_rmse = sqrt(test_mse)
test_rmse # test-rmse: 46675


```

```{r}
test_rmse / rf_benchmark


```
By tuning the hyperparamaters we have made a slightly greater improvement over random forest. It is however only a small improvement over the non tuned xgboost model. But these performance differences do matter in some circumstances!


## 3b. Efficiently tweak the hyperparamaters using a grid search/cross-validation

The caret package (short for classification and regression training) is used to simplify the grid search we just implemented. We can just pass it a grid of hyperparameter combinations and it will run all the combinations and do a cross-validation for each (so no validation set needed)

caret info

Similar to the tidyverse it works really well... but you have to learn all the code tricks!

```{r}
library(caret)

# look up the model we are running to see the paramaters
modelLookup("xgbLinear")

# set up all the pairwise combinations

xgb_grid_1 = expand.grid(nrounds = c(1000, 2000, 3000, 4000),
                            eta = c(0.01, 0.001, 0.0001),
                            lambda = 1,
                            alpha = 0)
xgb_grid_1


#here we do one better then a validation set, we use cross validation to 
#expand the amount of info we have!
xgb_trcontrol_1 = trainControl(method = "cv",
                                number = 5,
                                verboseIter = TRUE,
                                returnData = FALSE,
                                returnResamp = "all",
                                allowParallel = TRUE)


```
Train the model for each parameter combination in the grid, using CV to evaluate on multiple folds. Make sure your laptop is plugged in or RIP battery.
```{r}
######
#below a grid-search, cross-validation xgboost model in caret
######


xgb_train_1 = train(x = as.matrix(train_x),
                    y = train_y,
                    trControl = xgb_trcontrol_1,
                    tuneGrid = xgb_grid_1,
                    method = "xgbLinear",
                    max.depth = 5)

names(xgb_train_1)
xgb_train_1$bestTune
xgb_train_1$method
summary(xgb_train_1)


#alternatively, you can 'narrow in' on the best paramaters. Repeat the above by taking a range of options around 
#the best values found and seeing if high resolution tweaks can provide even further improvements.

xgb_cv_yhat = predict(xgb_train_1, as.matrix(test_x))


test_mse = mean(((xgb_cv_yhat - test_y) ^ 2))
test_rmse = sqrt(test_mse)
test_rmse # 46641... pretty close to the 'by hand' grid search!


```
Cam's hypothesis on caret performance - we are not using 'early stopping rounds' here so the model isn't cutting out at the exact best point. Re-running this with a validation setup as opposed to a cv setup would allow us to implement a grid search efficiently and wind up with the best hyperparamaters. I shall leave this as a follow up exercise for the curious.

# 4 Ensemble the models together

This is a good strategy for when accuracy is more important then knowing the best predictors.
```{r}
#y_pred_rf #random forest
#y_hat_valid #xgBoost with validation
#y_hat_xgb_grid #xgBoost grid search
#xgb_cv_yhat #xgBoost caret cross validation

length(y_hat_xgb_grid)


blend_pred = (y_hat_valid * .25) + (y_pred_rf * .25) + (xgb_cv_yhat * .25) + (y_hat_xgb_grid * .25)
length(blend_pred)

length(blend_pred) == length(y_hat_xgb_grid)

blend_test_mse = mean(((blend_pred - test_y) ^ 2))
blend_test_rmse = sqrt(blend_test_mse)
blend_test_rmse


```
Just by averaging 4 (very similar) predictors we have dropped the rmse a few percent lower then the best scoring of the 4 models. This does come at a cost though, we now can't make accurate inferences about the best predictors! The strategy is more effective when you take a more diverse set of models and ensemble those together.

next step - you can grid search the weights of the ensemble to try and drop the rmse further!
